# 训练的抽象层级

当你拥有了一批训练数据，我们对模型进行训练的时候，你到底在训练什么？

## 低层级视角

深度学习训练的过程也与机器学习一样，是根据梯度方向寻找最小化代价函数的过程。
在传统的机器学习中，我们通过特征方程写出模型 $f=w_1x_1+...+b$ 。我们期望优化参数，并使其能够预测真实结果。即，训练模型即寻找参数，使模型预测更准的过程。

深度学习中参数位于一个个的神经元中，我们将整个神经网络视为一个无比复杂的复合函数。那么单个神经元中的参数就相当于这个复杂函数中的某一层。通过复合函数求导，我们可以知道一个特定神经元中的参数下降方向。这也是反向梯度算法的原理。在这个过程中，我们仍将整个神经网络视为一个大 $f$

## 高层级视角

整个训练模型的过程，实则是使用**优化器**Optimizer优化参数，使其满足**目标**Objective的过程。

- 其中，Objective指我们的根本任务： $minL(\theta) = L(f_\theta(x),y)$，它度量预测和真实的差距。
- 其中，Optimizer指一类基于梯度的算法框架，比如 *Adam*。它指明了我们以何种方式优化参数。

一旦明确了这两点，我们就可以基于任何数据，做任何预测。

## 数学上

$f_\theta(x)$ 提供了一个函数空间，他决定了你能表示的函数族的复杂度。比如Transformer或CNN，实际上对应一个假设空间。数据 $(x,y)$ 提供了经验约束。