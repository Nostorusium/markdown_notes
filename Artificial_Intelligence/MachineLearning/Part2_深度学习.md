# 深度学习

## 总览

神经网络(Neural Networks)最初是为了模拟大脑工作所提出的，在上世纪八九十年代兴起，又在九十年代衰落，直到2005年才重新兴起。尽管一开始是为了模拟大脑这一生物目的被提出，但今天的神经网络与网络如何工作几乎无关。我们对大脑的生物学认识太过浅显，盲目地用如今对大脑的不充分认识来模拟大脑不见得会有成果。但我们从生物学灵感上得到了一个简化的数学模型：神经元(neuron)，接受输入并给出输出，输出又作为另一个神经元的输入。

因为历史关系的变化，现在我们一般叫它**深度学习**(deep learning)。为什么现在它又兴起了呢？

在数据量急剧增长的现代，传统的机器学习方案不能有效的利用这些庞大的数据（比如线性回归，逻辑回归）。但研究者们发现如果在同样的数据集下建立一个小神经网络，其性能表现往往要强于传统方案。对于一些特定类型的应用程序（数据庞大），建立更庞大的神经网络，其性能表现更好。

如果你能训练一个非常大的神经网络，利用你拥有的大量数据，你将可以在从语音识别到图像识别的任何事情上获得性能，而这是早期的机器学习算法做不到的。

## 多层感知机 Multi-payer Perceptron

假设你是一个淘宝卖家，你归纳了一件衣服的各种数据，并试图判断它是否会畅销。
这是一个二分问题，即答案只有是与否，我们可以用logistic回归来解决。比如，输入x为衣服价格，输出 $f(x)= \frac{1}{1+e^{-(wx+b)}}$ 。

为了建立一个神经网络，我们要稍微改变一下术语。我们使用小写字母a，activation，表示这个logistic回归的输出。这是神经科学领域的一个术语“**激活**”，表示一个神经元发送了多少对下游其他神经元的输出。

这个逻辑回归的小单元可以被认为是神经元的一个非常简化的模型：它接受输入x，并给出输出$a = f(x)= \frac{1}{1+e^{-(wx+b)}}$ ，它输出这件衣服畅销的概率。要建立一个神经网络，你只需要弄一大堆这样的神经元，并把他们连在一起。

而如果问题更复杂呢？假如你拥有不止价格，还拥有运费，营销费，衣服材质等参数呢？你可能会想，一件衣服的畅销程度可能与用户的购买力、用户感知、用户对质量的要求等有关。

我们可以制造一个专门用于评估用户购买力的神经元。它接受：衣服价格、运费参数，并给出一个输出。建立用户感知神经元，接受营销费用作为参数，也给出一个输出。种种方面都可以用这样的神经元来表示：他们接受若干输入，并给出一个输出。

最后我们只需要把这一大堆神经元的输出作为最后一个神经元的输入：我们期望他能给出该衣服畅销的概率。这就是一个神经网络模型。

在最开始的输入，与最终输出的概率之间，我们建立了若干神经元来评估购买力、用户感知、质量需求等。这一列，或者说一组神经元称为一**层**，layer。一层可能有多个或者单一神经元。

---

我们称给入特征x的一层神经元叫**输入层**(input layer)，最终输出结果的层级叫**输出层**(output layer)。夹在输入和输出中间的神经元层级，即**隐藏层**(hidden layer)。我们对向后的输入叫**激活**(activation)，而对他们的输入即**激活值**(activation values)。

要建立一个大的神经网络，你可能要手动决定哪些神经元应该接受哪些特征作为输入。在实际中，层与层之间是全连接的，你只需要适当的设置参数来让“判断购买力”的神经元重视价格输入，忽视营销费用，让神经元关注自己的功能。

我们可以再对这个模型做一次简化：输入层输入一组特征，即一个向量。中间层接受激活值，并给出激活向后传递。最终，我们得到输出，即：$\vec{x} \rightarrow \vec{a} \rightarrow a$

中间层之所以叫做隐藏层，是因为你用训练集做训练的时候，你的数据可以告诉你开头的**输入**与**输出**的结果，而你看不到中间层内部的输入和输出。

---

在机器学习部分，我们提到过特征方程工程。当时的例子是，使用房子的长度、宽度、深度，占地面积等等特征构建f，我们手动人工打造特征方程，充满了工匠精神。

而深度学习的好处就在于，你不需要手动设计这种特征方程。在上例中我们让隐藏层计算了：购买力，感知度，质量需求。而实际上，神经网络一个很好的特性是，当你用训练集训练它的时候，你根本不需要手动设计这些功能，整个神经网络会自行弄清它在隐藏层中需要什么。

隐藏层可以有很多层，在你设计自己的神经网络时，你需要做的一个决定就是你到底要多少个隐藏层，以及每个隐藏层里要多少个神经元，即选择一个足够好的**神经网络结构**(neural network architecture)，这将对算法的性能产生影响。

这种多层神经网络即**多层感知器**(multilayer perceptron)

>一个图像识别的例子：
>假如你弄了个多层神经网络来识别图像，有若干个隐藏层。
>当你看向某一层中的某个神经元，你可能会发现这个神经元专注于寻找竖线，另一个神经元专注于寻找横线。不同的神经元寻找不同方向的线条。
>当你看向下一层中的神经元，你发现他们试图学会把短线条组合起来，来寻找鼻子，眼睛。
>再看向下一层，可能他们开始学习将人脸的不同部分组合起来，检测人脸与不同脸型的对应程度，帮助输出层输出。
>神经网络能从数据中找出这些东西。

### 更复杂的情景

一个神经网络可以拥有很多层，通常输入层视为第0层，隐藏层的第一层视为第1层，并向后传递。

我们使用方括号上标，来标识第几层的值，比如 $\vec{a}^{[4]}$ ，表示第4层向后传递的激活向量。

每一层内有若干神经元，神经元接受输入，并给出输出a。一层神经元将给出一堆输出，这些输出作为一个**向量** $\vec{a}$向后传递，并成为下一层的输入。

$a_1^{[3]} = g(\vec{w_1}^{[3]} · \vec{a}^{[2]} + b_1^{[3]})$  表示第三层的第一个神经元，正在使用自己的参数w与b，结合上一层传递的输入 $a^{[2]}$ ，计算自己的激活值 $a_3^{[3]}$。
更一般地，第i层中第j个神经元接受上一层的输出 $a^{[i-1]}$ 作为本层的输入，并计算自己的输出 $a_j^{[i]} = g(\vec{w_j}^{[i]} · \vec{a}^{[i-1]} + b_j^{[i]})$

此处g还有一个名字，叫做**激活函数**(activation function)，因为它输出激活值a。在上文判断衣服畅销的二分例子中，这个激活函数可以是sigmoid函数。在其他情况里也会使用不同的激活函数。

## 前向传播

从前到后，从左到右，从0层到最后一层输出，即**前向传播**(forward propagation)。
与之相对的是**反向传播**(backward propagation)

