# 机器学习 Part1

## 总览

机器学习总的来说是寻求一种映射，将x映射为y。将一个给定的输入，映射到一个你所希望的输出。

机器学习可以分为两类：监督学习(Supervised Learning)，与无监督学习(Unsupervised Learning)。

### 监督学习

监督学习，即我们对他的学习过程进行监督，我们“监督”输出的正确与错误与否。

学习算法可以分为两类：回归(Regression)与分类(Classification)。

所谓回归，指预测的结果从无穷多的结果中选择，比如预测房价，股票。

所谓分类，指预测的结果是有限多的类别，比如预测肿瘤是恶性还是良性，预测动物是猫还是狗。

### 非监督学习

监督学习在数据集中寻找寻找一个结果，一种答案，比如肿瘤是否为良性。

而非监督学习不寻求这种答案，即不进行“监督”，它的工作是找到一些结构或者模式。

无监督学习算法可能决定将数分配到两个不同的组或者集群，称为聚类算法(cluster algorithm)。它将未标记的数据放入不同的集群中。
- 比如Google新闻，把相同内容的文章聚合在一个版块。这种聚类算法自行计算出哪些词暗示某些文章属于哪一组。新闻每天都在变化，该算法必须在无监督的情况下自行计算今天的新闻文章有哪些。
- 比如对DNA序列做处理，也许基因里的某一行表示眼睛的颜色，或者某人是否喜欢吃土豆。通过聚类算法把一类人聚合在一起从而分类。

非监督学习旨在寻求一种模式，没有提前人为给算法提供示例。
聚类算法作为一种无监督学习算法，获取没有标签的数据并尝试自动地将其分组到集群中。
还有其他几种非监督学习算法：
- 异常检测(Anomaly detection)，用于检测异常事件。比如金融系统中的欺诈检测。
- 降维(Dimensionality reduction)，是你将一个大数据集神奇的压缩成一个小得多的数据集，同时丢失尽可能少的信息。

## 线性回归 Linear Regression

监督学习中提到一个预测房价的例子，假如你现在要出售一套房子，你拥有一张面积与售价的数据集。要何预测你能够卖出多少钱？
一种方法是将数据拟合成一条直线，即**线性回归模型**，一种特殊类型的监督学习模型。

与回归模型相比，另一种最常见的监督学习模型成为**分类模型**，预测类型或离散类别，比如图片是猫或者是狗。
两者的区别在于**分类**的输出是有限的，而**回归**的输出是无限的。

### 一些术语

我们将数据集称为**训练集**(training set)，首先要训练模型在训练集中训练。

在机器学习中，表示输入的标准符号是小写的x，我们称之为**输入变量**(input variable)，**特征**(feature)，**输入特征**(input feature)。**输出变量**(output variable)的标准符号是小写的y，有时也称为**目标变量**(target variable)。我们使用小写字母m，表示训练示例的总数。

我们使用(x,y)表示一个具体的训练示例。比如(114,514)，对应输入114输出514。
如果你将数据集按行写成表格，你会得到类似下标的行号。$(x^{(i)},y^{(i)})$ 表示第几行的数据，i为下标。

回想一下，监督学习中的训练集包括输入特征和输出目标。输出目标是我我们希望模型能够学会的正确答案。要训练模型，我们需要将训练集提供给你的**学习算法**。你的监督学习算法会产生一些功能/**函数**(function)，我们将其写成小写的f。历史上这个函数曾被称为假设(hypothesis)，但现在我们称呼其function，f。

f 的工作是对新的输入 x 对输出进行估计和预测，我们称其 $\hat{y}$ (y-hat)。在机器学习中，惯例使用 $\hat{y}$ 表示y的**估计和预测**。而这个函数f，我们称其为**模型**(model)

模型的预测是y的估计值 $\hat{y}$ ，当符号使用字母y，它指的则是目标(target)，训练集中的实际真实值。

### 函数f

设计学习算法时，一个关键问题是该如何表示函数f，或者说我们要用来计算f的数学公式是什么？

若我们坚持f是一条直线，则表示为 $f_{w,b}(x) = wx + b$ 。我们暂时认为w,b都是数字，为w和b选择的值将根据输入特征x确定预测的y-hat。
简单表示，把下标省略，$f(x)$ 也是可以的。在统计学中，w通常来自weight权重，b通常来自bias偏置。

>该算法从数据中学习并生成最合适的线。此时你可能会问为什么选择线性回归？线性(linear)只是直线的花哨说法，为什么不选择非线性的函数，比如曲线或抛物线？实际上有的时候你也会想拟合更复杂的非线性函数，但是线性函数相对简单与易于使用，使用一条线作为基础，最终会帮助你获得更复杂的非线性模型。

这个特殊的模型即**线性回归**，更具体的说这是一个具有一个变量的线性回归，因为我们只有一个输入变量x：房子大小。我们也称它为**单变量线性回归**(univariable linear regression)。其中拉丁缀 *uni-* 表示一个。

### 代价函数

为了实现线性回归，一个关键步骤是定义一个**代价函数**(cost function)(成本函数)。

在模型 $f_{w,b}(x) = wx + b$ 中,w和b被称为模型的**参数**(parameter)。在机器学习中，模型的参数是可以在训练期间调整的。有时你也会听到参数被称为**系数**(coefficient)或**权重**(weight)。

对于线性回归，你要做的是选择参数w和b的值，以便让函数f得到的直线以某种方式很好的拟合数据。则$\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = wx^{(i)} + b$ 。 问题在于如何找到w和b的值，使 $\hat{y}^{(i)}$ 的预测更接近真实目标 $y^{(i)}$ ？为此，我们需要考虑如何衡量一条直线与训练数据的拟合程度。

我们将要构建一个**成本函数**(cost function)，它采用预测的y-hat与真实值y的**误差值** $(\hat{y}-y)$ 。
1. 将其平方得到平方误差 $(\hat{y}-y)^2$
2. 测量整个训练集的误差并将其加和，得到 $\sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^2$
3. 最后除以m，得到 $\frac{1}{2m} \sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^2$ 。这里为了后续的计算更简洁使用2m，但无论有没有这个2，该成本函数都有效。

最终我们得到成本函数 $J(w,b) = \frac{1}{2m} \sum_{i=1}^{m}(\hat{y}^{(i)}-y^{(i)})^2$
这也被称为**平方误差成本函数**，因为你所取得是误差项的平方。在机器学习中，不同的人会针对不同的应用程序使用不同的成本函数，但平方误差成本函数是迄今为止线性回归最常用的函数。

该成本函数也作： $J(w,b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2$
我们的目标即找到w,b使J最小。

### 确定代价函数的参数

当你在求成本函数，你在求什么？
- 你确定了模型，即 $f_{w,b}(x) = wx + b$ 。
- 你需要确定参数 w,b 来确定一条直线并使其能够很好的拟合训练数据。
- 你得到了成本函数 $J(w,b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2$ 来衡量预测值和真实值的差异。
- 我们的目标是最小化J，即 $minimize_{w,b} J(w,b)$。
 
我们先考虑一个简化的模型 $f_w(x)$ 与 成本函数 $J(w)$。省略b,或者令b等于0。
对于一个给定的w,他将确定一条过原点的直线，通过求与y-hat与y的误差最终得到成本函数J(w)。不同的参数值w共同构成了函数J(w)的图像，我们取其最小值。
而对于更一般 $J(w,b)$ 你需要同时确定w与b，这将造成一个二维函数，取 $z = J(w,b)$ 得到一张曲面，随后借助一些必要的数学工具，你或许能够得到其最小值。

可视化方法是极好的。
- 一种想法是直接将该曲面以三维形式展示出来，或许得到一个碗形，取碗底所对应的w,b。
- 又或者采用等高线的方式，像地形图一样水平切割这个曲面，得到一张等高线图，并确定碗底。在等高线图里你还能确定那些w,b会造成相同的代价函数。

## 梯度下降 Gradient Descent

在线性回归中，你不必手动尝试读取w和b的最佳等高线图，这是一个麻烦的过程。你真正想要的是一种高效的算法，可以用代码编写它来自动的找到参数w和b的值，并给出最佳拟合线。

有一种算法可以做到这一点，称之为**梯度下降**(gradient descent)，也是机器学习中最重要的算法之一。它在机器学习中无处不在，不仅用于线性回归，也用于最先进的神经网络模型，也称为深度学习模型。

梯度下降是一种可用于尝试最小化任何函数 $min_{w_1,...,w_n,b}J(w_1,w_2,...,w_n,b)$ 的算法，而不仅仅是线性回归的成本函数。使用梯度下降某种意义上是猜测w与b的过程，我们从一个初始w,b值开始，每次都稍微改变参数w和b来试图降低成本，直到 j 稳定或者接近最小值。值得注意的是，函数可能存在不止一个的最小值。

>回顾高数中的梯度grad
>方向导数是给定方向的变化率，是一个值。
>$f'_{（cos\alpha,cos\beta)} = f'_x(x,y)cos\alpha+f'_y(x,y)cos\beta$
>其中单位方向向量 $e = (cos\alpha,cos\beta)$，该方向指向 $(f'_x(x,y),f'_y(x,y))$ 时该值最大。
>梯度是一个向量，指向变化率最大的方向，即 $(f'_x(x,y),f'_y(x,y))$。当e取grad时，变化率最大。

想象你站在 $z=J(w,b)$ 曲面上，你试图寻找该曲面的最小值。你环顾四周，决定从最速下降的方向行动，即grad方向，每次走一小步。这就是梯度下降算法的过程。
你可以为参数w,b设置不同的初值，来确定不同的起始位置。但显然你所得到的是局部最优解，并非全局最优解。这是一种贪心算法。

### 梯度下降的细节实现

梯度下降的过程即不断更新w与b的过程。

$w = w-\alpha \frac{\partial}{\partial w} J(w,b)$
$b = b-\alpha \frac{\partial}{\partial b} J(w,b)$

其中， $\alpha$ 称为**学习率**(learning rate)，它通常是一个0到1之间的小正数，它用来控制你下坡的步长。一个大α意味着激进的下降，小α意味着小步下坡。
$\frac{\partial}{\partial w} J(w,b)$ 和 $\frac{\partial}{\partial b} J(w,b)$ 是代价函数的偏导。
对于梯度下降算法，要重复这两个更新步骤，直到算法收敛。

```
temp_w = updateGrad(w);
temp_b = updateBrad(b);
w = temp_w;
b = temp_w;
```

值得注意的是，对于w和b的更新要同步，不要先更新了一个后更新另一个，如果用代码表示则类似该例。

### 学习率 Learning Rate

一个很小的学习率意味着很慢的速度抵达收敛(converge)，因为步长太小。但你最终仍能抵达最小值。
一个很大的学习率意味着步伐很大，你可能会越过最小值，随后沿着梯度方向继续行走逐渐变大，最终无法收敛，甚至发散(diverge)。
当你迫近最小点，即便学习率不变，随着坡度放缓，偏导数本身也会随之减小。这意味着你下降的步长也随之变小。因此你最终仍能得到局部最优解。

### 线性回归中的梯度下降

$\begin{cases}
  w = w-\alpha \frac{\partial}{\partial w} J(w,b) \\
  b = b-\alpha \frac{\partial}{\partial b} J(w,b)
\end{cases}$

$J(w,b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})^2$
则：
$\frac{\partial}{\partial w} J(w,b) \\
= \frac{1}{2m} * 2\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) * x^{(i)} \\
= \frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) * x^{(i)}$

$\frac{\partial}{\partial b} J(w,b) \\
= \frac{1}{2m} * 2\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) * 1 \\
= \frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})$ 

于是得到：
$\begin{cases}
  w = w-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)}) * x^{(i)} \\
  b = b-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{w,b}(x^{(i)})-y^{(i)})
\end{cases}$

对于线性回归的**平方误差成本函数**，成本函数拥有全局单一的最小值，因为其函数图像为一个碗。

> 术语：**批量**梯度下降 **Batch** Gradient Descent
> 指每次执行梯度下降时，考虑所有的训练数据，而不是其中一部分。
> 有其他版本的梯度下降不会查看整个训练集，而是在每个更新步骤查看一个较小的子集。

在预测房价一例中，你的程序已可以通过梯度下降根据给定的初值w与b，逐步在碗的高处向最小点逐步逼近，最终收敛抵达拟合。
至此，我们成功完成了第一个机器学习模型。

## 多特征线性回归

在先前的例子中， $f_{w,b}(x) = wx + b$ ，我们输入房屋面积x，并得到y-hat这一预测结果。
而如果我们拥有不止一个**特征**，而是同时拥有：占地面积，卧室数量，楼层数，房屋年限等等多个特征？

我们引入更多符号：
- $x_1,x_2,x_3,x_4$ 或者 $x_j$ 表示多个特征。
- $n$ 表示特征总数。
- $\vec{x}^{(i)}$ 表示某一行$(x_1,x_2,x_3,x_4)$ (使用i,j区分行列)，有时也叫他**向量**(vector) 
  如$\vec{x}^{(2)}$ 表示第二个示例中的x。箭头是可以省略的，仅仅用来指示这是向量。
- $x_{j}^{(i)}$ 表示i行j列，一个特征的值。

于是得到新的模型：$f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 + b$
更一般地，我们将：
$\vec{w} = [w_1,w_2,w_3...w_n]$ 与 $b$ 作为该模型的**参数**。
$\vec{x} = [x_1,x_2,x_3...x_n]$ 为**特征**(输入变量)

简洁地，$f_{w,b}(\vec{x}) = \vec{x} · \vec{w} + b$
此为**多元线性回归**(multiple linear regression)，具有多个特征。

### 矢量化 Vectorization

显然向量的引入带来了极大的便利，数学上更是有着线性代数这一强大工具帮助我们处理问题。

$
\vec{w} = [w_1,w_3,w_3] \\
\vec{x} = [x_1,x_2,x_3]
$

```
//numpy 是python中最著名的线性代数库
w = np.array([1.0,2.5,-3.3])
b = 4
x = np.array([10,20,30])
```

在不引入向量的前提下， $f_{w,b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + b$
代码如下:
```
f = w[0]*x[0] + w[1]*x[1] + w[2]*x[2] + b
```
或者:
```
f = 0
for j in range(0,n)
f = f + w[j]*x[j]
f = f+b
```
这样的代码较为繁复，也不利于计算机硬件处理，库与硬件已经为向量运算做了很多优化。以下是一个向量化的版本：

```
f = np.dot(w,x) + b
```

代码简洁的同时，它的运行速度比不向量化的例子都要快，因为numpy本身已经做了并行优化。

### 多元梯度下降

代价函数 $J(w_1,...,w_n,b)$衡量拟合程度的形式不变，
 $J(\vec{w},b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2$
则：
$\begin{cases}
  w_1 = w_1-\alpha \frac{\partial}{\partial w_1} J(\vec{w},b) \\
  ... \\ 
  w_n = w_n-\alpha \frac{\partial}{\partial w_n} J(\vec{w},b) \\
  b = b-\alpha \frac{\partial}{\partial b} J(\vec{w},b)
\end{cases}$
偏导得：
$\begin{cases}
  w_1 = w_1-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)}) * x_1^{(i)} \\
  ... \\
  w_n = w_n-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)}) * x_n^{(i)} \\
  b = b-\alpha \frac{1}{m}\sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})
\end{cases}$

> **正规方程**(Normal Equation)是另一种求得最小化J的方法。
> 该算法只适用于线性回归，无需迭代即可得到w,b
> 缺点是它过于局限，不能推广，而且当n很大时非常慢。
> 在一些成熟的高级机器学习库中，在后端计算出w与b是可能的。但对于大多数情况下，梯度下降是更好的选择。

## 特征缩放与更多优化

考虑一个简单的例子：
$x_1$ 表示房屋面积，范围300-2000，范围较大。
$x_2$ 表示卧室数量，范围0-5，范围较小。
$price = w_1x_1 + w_2x_2 + b$
考虑一个这样的房子并试确定一个合理的参数：
$x_1 = 2000,x_2 = 5,price = 500K$ 

1. $w_1 = 50,w_2 = 0.1,b = 50$
   则： $price = 50*2000+0.1*5+50 = 100050.5K$ ，差距过大。
2. $w_1 = 0.1,w_2 = 50,b = 50$
   则： $price = 0.1*2000+50*5+50 = 500K$ ，接近结果。

因此，当一个特征可能的范围很大时，一个好的模型更有可能选择一个相对较小的参数。当特征可能得范围很小，那么参数的合理值会更大。

### 特征缩放与梯度下降

这涉及到梯度下降中的一个问题。假设 $x_1$ 范围大，那么参数 $m_1$ 设置的较小。 $x_2$ 与 $m_2$ 则相反。这会造成一个问题：同时拥有一个范围较大，一个范围较小的两个参数，导致图像变扁。如果图像的轮廓过于高瘦，如果学习率不变，梯度下降过程中可能要多次回弹，导致运行缓慢。

如果我们能够对训练数据执行一些转换，让参数更平均，那么我们得到的轮廓会更圆，下降更快。

一个简单的做法是，将每个 $x_i$ 都除以它范围的最大值。
则：$x_1$ 的范围将变成0.15~1， $x_2$ 的范围变成 0~1。

你也可以做所谓的**均值归一化**(mean normalization)，取 $x_i = \frac{x_i - \mu_i}{max-min}$
即：
$\begin{cases}
    x_1 =  \frac{x_1 - \mu_1}{2000-300} \\
    x_2 =  \frac{x_2 - \mu_2}{5-0}
\end{cases}$

或者**Z-score归一化**，计算每个特征的标准差 $\sigma$ 和平均值 $\mu$ ，令 $x_i = \frac{x_i-\mu_i}{\sigma_i}$ 。

>这个做法的依据来源于概率论，当样本足够多其分布将呈现出钟形曲线/高斯分布/正态分布。 $\frac{x-\mu}{\sigma}$ 将数据 x 和总体均值 μ 的差异以标准差 σ 为单位表示，统一了尺度。

作为经验法则，我们总是期望把特征重新放缩到-1~1。对于一些还可以的情况，不放缩也是没问题的。但对于过大或者过小的情况而言，我们仍然需要rescale。

### 梯度下降的收敛

梯度下降的过程即不断令 $J(\vec{w},b)$ 变小的过程。若以**迭代次数**与更新后的J画出函数图像，我们期望得到一个**递减**的曲线，这样的曲线我们称之为**学习曲线**(learning curve)。

如果曲线显示在某一次迭代后递增，即J没有降低，说明**学习率**选取并不理想(过大)。

随着迭代次数变多，曲线最终会变平，抵达**收敛**。我们难以判断到底需要多少次迭代才能抵达收敛，但画出这样的学习曲线有助于你决定何时停止模型的训练。

另一种决定何时停止训练的方法是采用**自动收敛测试**(automatic convergence test)，当某次迭代的变化小于 $\epsilon$ ，一个很小的数，则停止。但选取一个合适的 $\epsilon$ 也是一个难题。

### 学习率的选择

学习曲线如果忽高忽低，要么你写的代码有bug，要么可能你选择了一个过大的学习率。
> 当学习率过大，则下降的步长就过大，导致越过最小值又爬了上去，从而使J变大。

一个好用的办法是在一开始选取一个较小的学习率，如果在这样的情况下仍然有学习曲线忽高忽低，或者干脆不收敛的状况，很有可能是程序中存在bug。
这个行为只是作为debug的步骤之一，并不意味着要真正release，因为过小的学习率意味着过多的迭代。
从较小的学习率开始，多次尝试，寻找一个足够大的学习率。

### 特征工程 Feature Engineering

回到预测房价的例子，假设你拥有特征x1长度，特征x2宽度，你可能建立一个这样的模型：
$f_{w,b}(\vec{x}) = w_1x_1+w_2x_2+b$

根据你的直觉，你会觉得使用面积更能预测价格，于是你得到了特征 $x_3 = x_1x_2$
所以新的模型变成：$f_{w,b}(\vec{x}) = w_1x_1+w_2x_2+w_3x_3+b$

这样创建一个新的特性只是特征工程的一个例子。你可以根据你对问题的理解，或者干脆依靠你的直觉来设计新的特征。
这样新的特征通常是问题原始特征的转换或结合。

### 多项式回归 Polynomial Regression

线性回归不一定适用于所有训练集，你可以根据情况调整多项式函数，
比如你拥有特征x，你可以调整函数为二次函数 $f_{\vec{w},b}(x) = w_1x+w_2x^2+b$
或者 $f_{\vec{w},b}(x) = w_1x+w_2\sqrt{x}+b$

日后我们会探讨如何选择不同的特征与不同的模型，你会习得一个过程来衡量是否应该在函数里包括某些特征。

## 分类

不同于回归，分类的输出变量y只能接受少数几个可能之中的一个，而不是无穷范围内的数字。
事实证明线性回归不是分类问题的好算法，我们将引入一种新的算法：Logistic回归。

### 分类中的线性回归问题

以预测肿瘤为例，我们只有两种结果：良性或恶性，假与真，0与1。这种只有两种情况的分类称**二进制分类**(binary classification)

在数据集中使用线性回归拟合出一条直线，并确定一个阈值比如0.5，并计量该直线上该阈值所对应的肿瘤大小，随后得到一个分界。

如果直线十分理想，该值应当可以作为良性肿瘤与恶性肿瘤的分界，而这个分界依赖于所拟合的直线。
假如训练集中的数据并不理想，你所拟合出的线性函数与真实情况有所偏差，你所得到的分界也是有偏差的。

理想的直线即线性回归导致的**最佳拟合线**，由阈值确定的分类分界线称即**决定边界**(decision boundary)

### 逻辑回归 Logistic Regression

>虽然Logistic回归里带有回归二字，但它实际上是一个分类算法。
>logistic回归能够避免上文提到的线性回归中的问题，并且它输出的结果介于0与1之间。

**sigmoid函数**(S型函数)也叫**logistic函数**(逻辑函数)， $g(z)= \frac{1}{1+e^{-z}} , 0 < g(z) < 1$ 这个函数的特点在于，当z特别小，我们会接近0，当z特别大，我们会接近1。当z取0，得到0.5。

以先前线性回归模型中的函数为例： $f_{\vec{w},b}(\vec{x}) = \vec{x} · \vec{w} + b$ 。
1. 我们取 $z = \vec{x} · \vec{w} + b$，将其带入逻辑函数 $g(z)= \frac{1}{1+e^{-z}}$ ，得到一个在0~1之间的数。
2. 取$f_{\vec{w},b}(\vec{x}) = g(z) = g(\vec{x} · \vec{w} + b) = \frac{1}{1+e^{-(\vec{x} · \vec{w} + b)}}$

一个 $f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-(\vec{x} · \vec{w} + b)}}$ 的合理解释是，它接受x，并给出输出y等于1的概率。比如一个病人给出肿瘤大小x，并得到输出y=0.7，含义为病人有70%的概率为恶性肿瘤。

即 $f_{\vec{w},b}(\vec{x}) = P(y=1 | \vec{x};\vec{w},b)$ ，表示给定输入x，参数w，b条件下y=1的概率。

> 我们只是将原先的f的基础上包装一层sigmoid函数，将其转换为0~1的一个概率，并根据阈值确定一条边界。
> 实际上只是把原先的线性曲线转化为概率上的S型曲线。该曲线还满足平滑，可导，性质很好。
> 并没有什么特别的理由非要选sigmoid函数不可，但它足够简单与易用。

### 决策边界 Decision Boundary

上例中我们已经把 f 转换为一个0~1上的**概率曲线**，我们可以设置一个阈值来确定y-hat是0还是1，即概率要抵达多少才认为y是1。一个常见的阈值是0.5，即如果 $f_{\vec{w},b}(\vec{x}) \geq 0.5$，则 $\hat{y} = 1$

那么什么时候 $f_{\vec{w},b}(\vec{x}) \geq 0.5$ ？回看sigmoid函数，如果 $z \geq 0$ ，则 $g \geq 0.5$ 。即当 $\vec{x} · \vec{w} + b \geq 0$ 时 $\hat{y} = 1$，$z=0$将是分界线。

对于确定的参数，比如 $ z = x_1+x_2-3 = 0$，我们能够确定出一条直线。这条直线即**决策边界**，一侧表示良性，一侧表示恶性。

对于更复杂的非线性情况，比如 $z = w_1x_1^2+w_2x_2^2+b$，决策边界将类似一个圆。给定参数，比如$z = x_1^2+x_2^2-1$，则决策边界为 $x_1^2+x_2^2 = 1$，一个正圆。

我们还可以有更复杂的非线性形式，比如更高阶的多项式，并得到更复杂的决策边界。

### 逻辑回归的代价函数

目前，S型函数将原先线性的 f 转化为概率， $f_{\vec{w},b}(\vec{x}) = \frac{1}{1+e^{-\vec{x} · \vec{w} + b}}$ ，现在的问题是，对于一个给定的训练集，如何 $\vec{w}$ 与 $b$

这是原先平方误差成本函数 $J(\vec{w},b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})^2$ ，对于线性回归 $f = \vec{w}·\vec{x} + b$ 他拥有全局单一的最小值，其图像类似一个碗。而对于逻辑回归 $f = \frac{1}{1+e^{-\vec{x} · \vec{w} + b}}$ ，其图像上下波动，拥有很多局部最小值，这就不再适合梯度下降了。

现在我们需要一个不同的成本函数，来更好的适应梯度下降。我们暂时把1/2放在求和内。
$J(\vec{w},b) = \frac{1}{m} \sum_{i=1}^{m}\frac{1}{2}(f_{\vec{w},b}(x^{(i)})-y^{(i)})^2$ 
现在引入术语**损失**(loss)，表示为 $L(f_{\vec{w},b}(x^{(i)}),y^{(i)})$ 。目前 $L = \frac{1}{2}(f_{\vec{w},b}(x^{(i)})-y^{(i)})^2$。J的含义则变为**损失**加和的均值。

我们对损失函数进行一些修改，使这一系列损失函数的加和最后也能呈现出类似于线性回归代价函数其拥有唯一最小值的性质。

$L(f_{\vec{w},b}(x^{(i)}),y^{(i)}) =
\begin{cases}
  -\log{f_{\vec{w},b}(\vec{x}^{(i)})} \qquad \quad if \: y^{(i)} = 1 \\
  -\log(1-f_{\vec{w},b}(\vec{x}^{(i)})) \quad if \: y^{(i)} = 0 \\
\end{cases}$

逻辑回归输出的 f 范围在0~1之间， 这个负的log函数可以视为原log函数上下翻转，我们得到一个在1处为0，在0处为无穷大的函数。f输出y-hat为1的概率。

如果真实数据 $y^{(i)}$ 为1，那么当f趋于1时，即y-hat为1的概率趋于1时，L趋于0。反之，当f趋于0时，即y-hat为1的概率趋于0时，L趋于无穷大。这是因为我们已知y=1，当f越接近1，我们认为它越正确，损失越小。
当 $y^{(i)}$ 为0，函数图像左右对称并偏移1，此时f在0处为0，在1处为无穷大。同理，f趋于0时越正确，L趋于0。f趋于1时，L趋于无穷大。

最后我们得到了新的代价函数 $J(\vec{w},b)$ ， 一个关于参数w与b的函数。证明它是“凹”的超出了本课的范畴，暂不讨论，但他已经是一个适合用于梯度下降的形式了。

>如果继续沿用旧的形式，$(f(x^{(i)}) - y^{(i)})^2$ 表示一个概率值减去一个为0或为1的y，其中f表示y=1的概率。若y=1，那么f越接近1，说明损失越小。若y=0，那么f越接近1，说明损失越大。可见这个旧损失函数仍能执行“衡量误差”的职能，只可惜它的函数性质不够优秀，不够凹。

### 逻辑回归的简化代价函数

为了不频繁地**分支判断** $y^{(i)}$ 的情况来决定使用哪个形式的分段损失函数，我们使用下列这个数学上完全等价的式子，以方便在代码中使用。
$L(f_{\vec{w},b}(x^{(i)}),y^{(i)}) = -y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)}))-(1-y^{i})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))$

最终我们得到了新的代价函数，即损失函数的加和平均：

$J(\vec{w},b) = \frac{1}{m} \sum_{i=1}^{m} L(f_{\vec{w},b}(x^{(i)}),y^{(i)})$

这个新函数的提出源于统计学的一个原理：**最大似然估计**(maxium likelihood)，一个关于如何有效的找到不同模型的参数的一个想法。他有一个良好的性质：他是凹的。
你无需关注最大似然估计背后的统计学考量，因为我们希望更多的考虑工程上的问题，而不是其背后的理论。

>根据教材地区的不同，凹凸性的定义有时是相反的。
>国内的凹凸性对于国外可能是凸凹性。
>本文中的凹，对应国外的凸(convex)。

现在我们可以不受阻碍的继续梯度下降了。

## 过拟合 Overfitting

对于一个拟合不佳的 $f = wx+b$，我们使用术语**欠拟合**(underfit)，表示它有很大的**误差**(bias)。

现在考虑：你成功创造出了一个f，它是一个高阶多项式函数，能够完美的适配训练集中的所有数据。而这样一条符合数据的扭曲曲线由于过度盲目的追求数据的拟合，反而失去了泛化推广的能力，我们称**过拟合**(overfit)。另一个术语是，该算法具有**高方差**(high variance)

其背后的逻辑是，你的算法过分努力的拟合每一个训练示例，结果只要稍微出现一些反例，你所得到的最终函数都可能完全不同。我们现在的目标就是找到一个既没有高误差，也没有高方差的模型。

### 正则化 Regularization

比起 $w_1x+w_2x^2+b$ 这个二次多项式，显然 $w_1x+w_2x^2+w_3x^3+w_4x^4+b$ 这个高次复杂多项式更容易陷入**过拟合**。

我们有一种方法可以使参数w3与w4很小，比如令其接近0。我们对成本函数稍作修改：

$J(\vec{w},b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})^2 + 114514w_3^2+1919810w_4^2$

我们总是寻找成本函数的最小值，这样的修改实际上是一种**惩罚**(penalization)，以希望选择更小的w3与w4，以至于其接近0。这样这个高次多项式就更接近前面的二次多项式，减轻了过拟合。

正则化的想法即，如果参数拥有一个较小值，意味着一个更"简单"的模型，更不容易过拟合。上例中我们只正则化了w3与w4。但更普遍的是，我们对所有特征都做正则化，因为你往往不知道哪些特征比较重要，哪些特征需要惩罚。

$J(\vec{w},b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{\vec{w},b}(x^{(i)})-y^{(i)})^2 + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2$

希腊字母 $\lambda$ 也叫**正则化参数**(regularization parameter)，类似于梯度下降中的学习率 $\alpha$ 需要被选择一个值。至此，一个代价函数被分成两部分：**误差均方部分**与**正则化部分**。你会发现这个正则化部分里没有写入b，因为b作为bias偏置影响不大，加不加都行。

该如何选择λ？如果λ为0，这意味着你根本没有做**正则化**，很可能陷入过拟合。而如果λ又过大，我们会倾向于选择过小的参数，比如0，此时f为一条直线f=b，导致**欠拟合**。我们希望找到一个不大也不小的值。

### 正则化线性回归