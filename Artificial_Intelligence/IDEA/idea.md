# 时序预测的一些想法

大论文不要求创新，只是要求"新"
需要综述能力写国内外研究（第二章）以及两个工作量。
第一个工作量，可以是baseline+A+B，缝两个模块上去。
第二个工作流，就继续堆，再加两个模块。
最后是讲故事的能力以及按照要求填入稀奇古怪的东西。

## rua

DLinear的出现已经说明了这个方向很水。不要深究。就缝。

8个数据集：pems ett electricity weather exchange rate
至少在公开数据集训练。
然后试一试私有数据集。

至少跑通一个基线，然后魔改

上因果推断，随便发二区。

结合LLM感觉不太靠谱，看看文章就行。

多变量/多模态时序预测？Informer的多变量只是简单的把其他类型数据一同当做特征输入。并且它输出的时候采用一步预测而不是多步迭代。

时序重点不在模型本身，反而在数据集本身的质量。多数公开数据集都不大，跑得很快。

也可以往异常靠一靠。总之非常好水，除非你要发顶刊。

~~Autoformer~~

>Transformer 最初是为自然语言处理 （NLP） 而开发的，将它们应用于时间序列分析需要进行适当的调整以适应时间序列的领域。 与NLP中单个词标记所携带的丰富语义信息不同，时间序列中的单个数据点通常与其相邻值相似，缺乏实质性信息[178]. 因此，基本模型的点注意力机制没有考虑可能跨越多个连续时间步长的更广泛的上下文或模式，而只计算单个时间步长的注意力，这使得捕捉时间序列数据的特征变得困难。 因此，考虑周围的上下文以及单个时间点可以在时间序列数据中提供更多信息。
>修补是指将输入序列划分为多个修补的技术，如图所示。10. 这种方法保留了每个补丁中的信息，从而增强了局部性。 通过处理补丁而不是单个点，模型处理的标记更少，从而降低了注意力机制的计算复杂性。 这种方法有助于克服预测性能下降的问题，当使用稀疏注意力来提高自注意力效率时，可能会发生这种情况，可能会丢失关键信息。

---

>在多变量时间序列预测中，了解变量之间的关系对于提高预测准确性至关重要。 直观地说，较高的温度会导致冰淇淋销量增加，这表明变量之间存在关系。 尽管存在这种明显的相关性，但令人惊讶的结果表明，独立处理通道的模型（例如 LTSF-Linear、PatchTST 和 PETformer）通常优于考虑通道间相关性的模型。 这一结果意味着当前模型无法有效地捕获变量之间的关系。

---

>曼巴成型机 [276]是一个将 Mamba 与 Transformer 解码器框架相结合的混合模型。由于 Mamba 块自然地内化了序列顺序信息，因此不需要位置嵌入。长期依赖关系通过 Mamba 块学习，而短期依赖关系通过自注意力层捕获，有效捕获整体依赖关系。这种方法克服了注意力机制的计算效率限制。双曼巴+ [138]集成修补技术以精细学习数据中的相互依赖关系。为了保存长期信息，它引入了 Mamba+ 块，它为 Mamba 块添加了一个遗忘门。它还采用 Bi-Mamba+ 编码器来双向处理输入序列。使用 Spearman 相关系数，系列关系感知 （SRA） 决策器旨在自动选择通道标记化策略（CI 或 CD）。DTMamba [272]由Dual Twin Mamba块组成，有效独立学习时间序列数据通道中的长期依赖关系。每个 Twin Mamba 块由两个并行的 Mamba 结构组成，它们处理相同的输入数据以有效地捕获不同的模式。一种曼巴结构学习详细的模式和短期变化，而另一种则学习整体模式和长期趋势。

---

>在 TSF 中，采用各种因果推断方法来识别数据中的因果关系。格兰杰因果关系检验 [85]检查一个变量的历史信息是否有助于预测另一个变量的未来。该方法基于回归分析检测因果影响的方向。然而，它在完全排除由第三个变量的存在引起的间接相关性方面存在局限性。结构因果模型 （SCM） [192]利用因果图和结构方程对变量之间的关系进行建模。这些模型支持干预模拟和反事实分析，提供因果关系的可视化表示，以帮助解释和整合多个变量之间的相互作用。做微积分 [190]是一种干预分析技术，通过计算特定变量的变化对其他变量的影响来定量分析干预的效果，从而支持因果预测。倾向评分匹配 （PSM） [207]是一种通过匹配具有相似特征的群体来进行因果推断的方法。这种方法最大限度地减少了混杂因素的影响，并更准确地评估了干预措施的效果。有向无环图 （DAG） [191]是方向性的、非循环的图，直观地表示因果关系。DAG 有助于清楚地识别因果结构并理解变量之间的复杂相互作用。人们正在进行各种尝试，通过利用这些不同的因果推断方法来增强 TSF 的因果关系[210,215].

## idea

1. 关于多变量预测，尽可能利用数据
2. 关于因果推断缝合
3. 关于Attention的改善
4. 有限的泛化性，与具体领域结合。
5. Mamba? 和transformer结合
6. 参考autoformer 分解合并