# Transformer

谷歌在2017年推出的原版Transformer专注于将一种语言的文本翻译成另一种。而我们要关注的变种，即用于构建ChatGPT等工具的模型，则是输入一段文字，甚至一些图片或语音，来预测出后文的内容。其结果展示为不同文本片段的概率分布，并选择概率较高者输出。

给他一个原始片段，从它给出的概率分布中取一个片段追加到文本末尾，再用所有文本进行新一轮的预测。这个重复预测和抽样的过程正是ChatGPT或其他大语言模型进行交互时遇到的。

GPT:Generative Pre-trained Transformer

## 结构总览

首先，输入内容会被切分为许多小片段token，它们可以是单词，单词片段，或者某个符号。每个token都对应一个向量，这个向量设法编码这个token的含义。如果将向量视为高位空间的坐标，那么意思相近的词，对应的向量也往往相近。

这些向量随后经过**注意力模块**处理，这使得向量能够相互交流，通过互相传递信息来更新自己的值。比如机器学习中的model一词和时尚杂志中的model一词明显有不同含义，注意力机制需要找出上下文中的哪些词会改变哪些词的含义，以及将他们更新为什么含义。这种含义已经通过某种形式完全的编码进了向量中。

随后，这些向量会经过另一种处理。根据不同资料，有些会叫他**多层感知器**MLP，在这个阶段向量不再相互交流，而是并行的接受同一种处理。在这个阶段更像是询问每个向量一些问题，并根据这些问题的答案来更新向量。


随后重复这个过程，令注意力模块与MLP模块层层堆叠，我们的目标是让整段文字的所有关键含义都以某种方式融入到序列的最后一个向量。我们最后对最后这一个向量进行某种操作，得到所有可能的token概率分布。

只要你能够根据初始文本预测下一个词，那么你就可以反复进行预测，抽样，追加的过程套娃生成一大堆。

---

GPT-3使用了 *175,181,291,520* 个参数，可以被组织为 *27,938* 个参数矩阵。这些矩阵又分为八类。我们需要了解不同类型矩阵的作用。

## Embedding

在分割token时，模型拥有一个预设的词汇库，包含所有可能的词汇。我们将遇到的第一个矩阵叫做**嵌入矩阵**(Embedding Matrix)，每个词都对应一列，这些列向量决定了每个token对应的向量，我们将其记为 $W_E$ 。它的初始值随机，就和其他矩阵一样，但将基于数据学习。

早在transformer之前，将单词转化为向量就是机器学习中的常见做法。我们通常称token**嵌入**(embedding)向量vector，这让你倾向于从**几何角度**理解他们。

当模型在训练阶段调整权重，确定不同单词该如何嵌入向量时，他们最终的**嵌入向量**在高维空间中的**方向**往往具有某种**语义**意义。

如果你取女人和男人的向量之差 $E(woman) - E(man)$ 并将其视为一个空间中的小向量，这个向量差将与女王与国王的向量之差 $E(queen) - E(king)$ 非常相似。假如你不知道 $E(queen)$ ，那么你可以令 $E(queen) \approx E(king)+E(woman)-E(man)$ ，理论上这是可行的。



![image not found](./resources/images/embedding_example.png)

$ E(Hitler)+E(Italy)-E(Germany) $ 很有可能得到 $E(Mussolinis)$，就好像模型学会了将某个方向与国家联系起来，另一个方向与二战领导人联系起来一样。

两个向量的点积可以视为衡量他们对齐程度的一种方法。如果向量方向接近，点积为正，正交则为0，反向则为负。这可以用来衡量两个token的关联程度。

---

一个嵌入向量不仅编码了一个token，也编码了token的位置信息。更值得注意的是，这些向量能结合上下文语境。比如一个token的嵌入向量可能被网络中的各个模块相互拉扯，最终指向一个更具体细致的方向。

在一开始根据输入文本建立向量组时，每个向量都是直接从嵌入矩阵中拉出来的，此时他们只是编码，而没有上下文信息。而Transformer的主要目标就是让这些向量能够获得比单个token更丰富具体的含义。 

这种网络一次只能处理特定数量的向量，叫做他的上下文长度。GPT-3的上下文长度为2048，即2048列token，每列有12000维。上下文长度限制了Transformer在预测下一个词时能够结合的文本量。

## Unembedding

上下文中的最后一个向量，将被映射到一个列表，每个值对应词库中的一个token。 这个用来映射的矩阵叫做**解嵌入矩阵**(Unembedding Matrix)，每一列对应一个嵌入维度。

他的行列与嵌入矩阵对调，我们记作 $W_U$，也在训练中学习。最后其输出经过softmax归一化，得到一列token的概率分布。

在softmax输出时，我们也会做一些特殊处理。

\[
    Softmax(x_i) = \frac{e^{x_i/T}}{\sum_{n=0}^{N-1}e^{x_n/T}}
\]

我们有时会加个分母T，称**温度**。当T较大时，他会给低值赋予更多权重，使分布更均匀。如果T较小，那么意味着所有权重都给到最大值。

如果T=0，意味着模型总是选择概率最大的token。而一个更大的T意味着模型愿意选择可能性较低的词，让他更新颖，也更有风险，容易说胡话。API通常不会让你使用超过2的温度。

## Attention

Embedding将嵌入向量嵌入高维空间，对应某种语义。Transformer的目标是逐渐调整这些嵌入，使其融入更丰富的上下文语义。
一个训练的好的注意力模块能够计算出，给初始由token得到的嵌入向量加个什么向量，使其移动到上下文对应的具体方向上。注意力模块允许模型相互传递这些嵌入向量所蕴含的信息。
而新嵌入向量的信息，可以比单个token更加丰富。

在注意力模块与MLP的套娃后，最后一层经过softmax输出概率分布时，其完全基于序列中的最后一个向量。最后一个向量必须经过所有注意力模块的更新，来包含整个上下文窗口。

### 单头注意力 Single head Attention

嵌入向量编码了token的含义与位置信息，和上下文没有关联，记为 $\vec{E}$，我们的目标是经过一系列计算，产生一组新的更精准的嵌入向量 $\vec{E}'$。
模型的实际行为往往是很难解释的，因为它只是通过调整大量参数来最小化某个函数，但为了方便解释，我们以名词与形容词为例进行说明：

每个名词都在询问：我前面有形容词吗？这样的询问被编码为另一个向量，称这个词的**查询**(query)。它的维度通常更小。

要计算一个查询向量，需要先取一个矩阵 $W_Q$， 由$ W_Q \vec{E_i} = \vec{Q_i}$。查询矩阵与文中所有的嵌入向量相乘，给每个token都计算出一个查询向量。

>查询矩阵的具体行为是从数据中习得的。实际来看，这个矩阵的作用非常难以解释。我们可以假设这个查询矩阵，将嵌入空间中的名词映射到较小的查询空间中的某个方向，用向量来编码“寻找前置形容词”这样的概念。

与此同时我们需要另一个矩阵，**键矩阵**(Key Matrix)，$W_k$ 与嵌入向量相乘，得到一系列 $\vec{K_i}$。从概念上讲，可以把键视为想要回答查询。
和查询矩阵一样，它将嵌入矩阵映射到较小的键空间，当查询与键的方向相对齐，我们认为他们相匹配。

为了衡量键与查询的匹配程度，我们需要计算两两点积，其值越大，则键与查询越对齐。用机器学习的术语来说，若点积较大，意味着前面token的嵌入**注意**到(attend)后面token的嵌入。

---

同一列token序列对应同样多的key向量与query向量，形成一个由其点积构成的方阵。这个点积的大小表示每个词语其他词的含义的相关性。

这些点积的用法是对某一列嵌入向量进行加权求和，得到一个权重，称**相关性**。

这样一来，数值就不能是正负无穷，而要介于0和1之间，而且每一列查询的加和为1。
因此对于每一列查询所对应的点积，我们将其投入softmax进行归一化，此时我们得到的介于0~1之间的数值，就可以看做权重，表示当前查询与若干个key的相关度。

现在得到的网格被称为**注意力模式**(attention pattern)

Transformer的论文中有一个简洁的写法：

$$
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V  
$$

这里的Q与K，包含了所有的query向量和key向量。由于原论文使用行向量，所以转置放在K上，但你只需要知道分母所得到的正是点积矩阵。
为了数值的稳定性，建议所有的点积除以key-query空间维度的平方根来平滑概率分布，以防止概率分布被拉扯成极值。

>一个技术细节
>在训练过程中，对给定示例跑模型的时候，模型会根据正确预测出下一词的概率高低进行奖惩并调整权重。
>而一个效率更高的做法是，在预测下一个词是什么的同时，也让他预测每个token子序列之后所有可能的下一个token。
>这样一个训练实例就能提供更多次训练机会。

就注意力机制而言，这意味着不能让后一个词影响前一个词，不然就会泄露答案。所以我们希望后方token影响前方token的点积能够被置为0。

为了仍然保证归一化性质，常见的方法是在softmax之前将其设为负无穷，再由softmax转化为0.
这一过程即**掩码**(masking)。有的注意力机制也不使用掩码。

关于注意力模式，值得一提的是，它的大小是**上下文长度的平方**。
这也是为何上下文长度会成为瓶颈。近年来，注意力机制出现了一些变体，旨在使上下文更具拓展性，但他们不在本次讨论范畴。

---

现在该更新嵌入向量了，把每个token的信息传递给与之相关的其他词。

这需要用到第三个矩阵，**值矩阵**(Value Matrix)，$W_V$。值矩阵乘以嵌入向量，得到**值向量** $\vec{v}$，作为你要给后词的嵌入所加的向量。因此这个值向量与嵌入向量位于同一个高维空间。

你可以将这个变换理解为：如果这个词需要调整目标词的含义，对目标词的嵌入需要加上什么向量。

现在，由嵌入向量你可以得到一系列值向量，我们由点积得到一系列权重。我们进行加权求和，得到原嵌入向量的变化量，记为 $\Delta E$。

$$
\vec{E_i}' = \vec{E_i} + \Delta \vec{E_i}
$$

将其加在嵌入向量上，完成更新，预期得到一个更精准的向量。

---

这一系列过程就是单头注意力机制。这个过程由三种充满了可调参数的矩阵实现： $W_Q,W_K,W_V$

### 多头注意力 Multi-headed Attention

Transformer内完整的注意力模块，由**多头注意力**(multi-headed attention)组成，大量并行地执行这些操作。每个头都有不同的K，Q，V矩阵。

$$
\vec{E_i} + \sum_{j=0}^{95}  \Delta \vec{E_i}^{(j)}
$$

GPT-3每个模块内使用96个注意力头，这意味这有96种不同的K，Q，V矩阵，产生96种不同的注意力模式。你要做的就是把每个头的变化量一起加到嵌入向量上。 

>目前我们讨论的都是**自注意力**(self attention)头
>这与其他模型中的变体**交叉注意力**(cross-attention)头有所区别。交叉注意力通常会处理两种不同类型数据，比如原文与译文，语音音频与转录文字。
>唯一的区别在于交叉注意力的 key-query 矩阵作用于不同的数据集。比如key可能来源于一种语言，query来源于另一种。此时通常不会使用掩码，因为不需要考虑后影响前。

通常而言，值矩阵的参数量等于键矩阵和查询矩阵之和，这有助于并行运行多个**注意力头**(attention head)。其具体做法是，将值矩阵分解为两个小矩阵相乘以降低复杂度，在线性代数上，这种操作叫做**低秩分解**(low rank transformation)。

![](./resources/images/低秩分解.png)

**3b1b**将这两个分解出来的矩阵称为**值↑矩阵**与**值↓矩阵**(value Up/Down matrix)，不同的论文里会有不同的称呼。从概念上讲，我们仍把这两个矩阵视为一个线性变换。

右面这个行少的矩阵，其行通常等于key-query查询空间的维我们将其看作是将一个较大的嵌入向量，降维到较小的空间，即*Value Down Matrix*。左侧的矩阵，则将其从小空间映射回嵌入空间，得到用于实际更新的向量，即*Value Up Matrix*。

这些值↑矩阵，有时会合并在一起，称**输出矩阵**(output matrix)，与整个多头注意力模块相关联。而单个注意力头的值矩阵，则单指第一步的矩阵，即将嵌入向量投影到低纬度空间的**值↓矩阵**。

## MLP

