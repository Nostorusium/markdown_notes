# 网络演变

|Linux   |0.96            |<2.1  |2.1 |2.5
|--------|----------------|------|----|-----|
|--------|支持网络阻塞/非阻塞|select|poll|epoll|

随着网络连接量级的的提高，网络功能的演变如上。

## 阻塞IO(BIO)

阻塞IO,Blocking IO.
网络应用调用recv(read)系统调用后，陷入内核要经过两个阻塞。
在内核态等待数据准备完毕一次，把准备完毕的数据拷贝回用户态一次。

阻塞IO下，通常一个client连接分配一个线程进行处理。
但缺点是能支持的并发client连接数比较少。一台server能分配的线程是有限的，而且大量线程造成的上下文切换也会影响性能。

## 非阻塞IO(NIO)

之所以一个client分配一个线程是因为处理客户端的读写是阻塞的，为了不影响接受后续连接将阻塞逻辑交给单独的线程。
之前的阻塞IO是因为客户端的读写是阻塞的，那么只需要把它改成非阻塞就可以了。因此Nonblocking IO需要内核支持：应用程序会不断地调用recv查看内核中的数据情况(缓冲区)，然后返回一个状态。如果有数据，那么才会执行拷贝，然后返回OK给应用，读写成功。

阻塞IO和非阻塞IO的主要区别在于内核中的数据尚未就绪时的处理。
BIO的做法是一直阻塞直到数据就绪。NIO则是返回一个错误状态，等待后续再次访问。

```
int socket(int domain, int type, int protocol);

SOCK_STREAM     Provides  sequenced,  reliable,  two-way,  connection-based byte streams.  An out-of-band data
    transmission mechanism may be supported.

SOCK_NONBLOCK   Set  the O_NONBLOCK file status flag on the open file description (see open(2)) referred to by
    the new file descriptor.  Using this flag saves extra calls to fcntl(2) to  achieve  the  same
    result.
```

socket的第二个参数type可以指定SOCK_NONBLOCK
将socket设置成非阻塞后，就可以通过一个线程管理多个client连接。但缺点就是需要不停地轮询内核数据是否就绪。
者设计了很多无效的、频繁的系统调用。

## IO多路复用:select poll

>要注意select,poll,epoll可以自由设置为阻塞/非阻塞
>但是epoll在ET工作模式时必须使用NIO模式。

为了改进非阻塞IO的不断询问，一个思路就是改成不主动询问，而是等待通知。
并且随着客户端连接的增加，系统调用的次数也随之增加，我们希望有一个能批量通知的功能。
这就引出了IO多路复用第一个版本：select和poll

select的思想就是，如果我们能够预先传入一个socket列表，如果列表中的socket都没有数据，那就挂起进程，直到某个socket收到了数据，才唤醒线程。

当程序调用select，该表示socket列表的fds传入内核，kernel内核轮询检查一次所有select负责的fd，若能找到已准备好读写的socket,返回。不然，阻塞挂起进程/线程，直到某个socket收到了数据。

在select的实现中，fds由位数组fdset表示，每一位表示一个描述符/一个已连接的socket的可用情况。

```
void FD_ZERO(fd_set *fdset) // clear all bits in fdset
void FD_SET(int fd,fd_set* fdset); //turn on the bit for fd in fdset
void FD_CLR(int fd,fd_set* fdset); //turn off the bit for fd in fdset
int FD_ISSET(int fd,fd_set *fdset); // check if the bit for fd on in fdset
```
几个接口表示归零、打开某个fd的set，关闭某个fd的set，查看某个fd是否已打开。

```
int select(int nfds, fd_set *readfds, fd_set *writefds,
                  fd_set *exceptfds, struct timeval *timeout);

nfds   This argument should be set to the highest-numbered file         descriptor in any of the three sets,  plus  1.
              The indicated file descriptors in each set are checked, up to this limit
readfds
              The file descriptors in this set are watched to see if they are ready for reading.
writefds
              The  file  descriptors in this set are watched to see if they are ready for writing.
exceptfds
              The file descriptors in this set are watched for "exceptional conditions".
timeout
              The  timeout  argument  is  a timeval structure (shown below) that specifies the interval that select()
              should block waiting for a file descriptor to become ready.  The call will block until either:

              • a file descriptor becomes ready;

              • the call is interrupted by a signal handler; or

              • the timeout expires.
RETURN VALUE
       On success, select() and pselect() return the number of file descriptors contained in the three  returned  de‐
       scriptor  sets  (that  is, the total number of bits that are set in readfds, writefds, exceptfds).  The return
       value may be zero if the timeout expired before any file descriptors became ready.

       On error, -1 is returned, and errno is set to indicate the error; the file descriptor sets are unmodified, and
       timeout becomes undefined.
```

select是把读事件和写事件分开管理的。可见参数有3个fd_set,分别表示写，读，异常。允许置NULL。

```
struct timeval{
    long tv_sec;    //seconds
    long tv_usec;   //microseconds
}
```

关于超时有三种可能：
1. 永远等待：如果timeout参数为空
2. 等待给定时间：等待指定的fd准备好IO,但等待超出给定时间则认为超时。
3. 不等待：立刻返回，给定时间为0。相当于轮询。

select使用固定长度的位数组，所支持的fd个数是有限的 (FD_SETSIZE) 默认最大值为1024，即0~1023的fd。
poll不再使用位数组，而是改用动态数组/链表来组织，突破了FD_SETSIZE个数限制，但仍然会受系统文件描述符的限制。但是poll和select没有太大的本质区别，都是使用线性结构来存储socket集合，需要遍历集合找到可读或可写的socket，时间复杂度O(n)。还需要用户态和内核态之间拷贝fd集合。

```
#include <poll.h>
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

struct pollfd {
         int   fd;         /* file descriptor */
         short events;     /* requested events */
         short revents;    /* returned events */
};
```

nfds传入的是数组长度，表示管理的描述符个数。因为fdarray是一个可变长度的数组，因此需要指定数组长度。

select和poll原理上没有太大差别。
select使用位数组，而poll使用pollfd动态边长数组管理。
select默认大小是FD_SETSIZE(1024)，该参数的修改需要重新编译内核。
poll因为边长数组理论上可以支持海量链接。
不管是select还是poll都需要从用户态拷贝管理的描述符fds到内核态，返回时都需要从内核态拷贝描述符回用户态，再由用户态遍历判断哪些就绪。

优点： 利用了一次系统调用select/poll就可以管理多个client事件(read,write,accept)
缺点： 需要将注册管理的多个client连接从用户态拷贝到内核态。在管理较多连接时由拷贝带来的资源开销较大，影响性能。


>从进程阻塞的角度看select
>当执行了select，操作系统会把当前进程加入到等待队列中去。
>当其中任何一个socket收到了数据，中断程序唤起进程，把该进程从等待队列中移除加入工作队列。
>这样一来，进程被唤醒后就知道至少有一个socket接受了数据。程序只需要遍历一遍socket列表就知道哪个socket就绪了。
>简单的方法往往有缺点，主要问题在于：
>每次调用select/poll都要将fds传入内核,唤醒时将更新的fds传递回用户态。
>遍历的操作开销大，因此select才会规定最大监视数量为1024

## IO多路复用：epoll

> 顺便一提poll可以翻译为轮询
> epoll就是event poll，事件轮询

IO多路复用第一版的问题是需要复制。
select/poll每次调用，都需要拷贝所管理的全量的fd到内核态，然后由内核态发起模糊通知。
即应用程序只知道有socket的读写操作就绪，而不知道具体是哪个，需要遍历寻找可读或可写的socket。
因此第二版多路复用做出了以下改进：
1. 避免拷贝：在调用过程中不传来传去。
2. 明确通知：避免遍历数组。

epoll即select/poll的加强版本。

```
int epoll_create(int size)

    creates a new epoll instance and returns a file descriptor referring to that instance. 
```

epoll_create()创建一个一个epoll实例，返回对应的fd。它在内核中分配一段空间，初始化用于监听的数据结构：红黑树和就绪链表等。
size用来指定监听的数目。这个参数只是指示这个epoll会处理事件的大致数目，而不是能够处理的最大数目。在较新的Linux版本中该参数没有任何意义。

```
int epoll_ctl(int epfd,int op,int fd,struct epoll_event* event);
```

epoll_ctl为暴露给上层的增删改接口，参数含义:
- epfd: 哪个epoll fd
- op: operation: EPOLL_CTL_ADD/EPOLL_CTL_MOD/EPOLL_CTL_DEL
- fd: 待监听的fd
- event: 要监听的fd事件(read,write,accept等)
即：将哪个客户端fd的哪些事件event交给哪个epoll来管理(op增删改)

```
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

从就绪事件链表中获取socket对应的fd。

### epoll分析与对比

select低效的一个原因在于程序不知道有哪些socket收到了数据，只能一个个遍历。
socket对象除了发送缓冲区，接收缓冲区，还包括一个等待队列结构，指向所有等待socket事件的进程。
如果内核维护一个就绪列表，引用收到数据的socket，就能避免遍历。其过程如下：

1. 当某个进程调用epoll_create创建eventpoll对象，他就创建了一个fd事件表，并返回fd描述符。
   此外还有一个rdlist链表，用来保存就绪事件。

2. 使用epoll_ctl把fd添加/修改/删除到这个事件表。添加fd时需要指定一个event参数，表示在该fd上监听此类事件。
   epoll在fd的基础上引入了事件的概念，与其说我们在监听fd，倒不如说我们在监听事件的发生。
   一个事件可以由事件类型(比如EPOLLIN表示数据抵达)和fd(表示该fd上发生的事件)构成。

3. epoll_ctl会为中断处理程序注册好回调函数。当某个socket数据到来，发生中断，并执行回调函数。
   该函数将epitem插入到rdlist就绪链表，表示该事件已发生。

4. epoll_wait以阻塞方式从rdlist获取一组事件。如果epoll_wait检测到事件就绪，就将所有就绪的事件从事件表拿出，并以回调形式复制到参数所指定的数组中。若为空则阻塞等待。参数给定的数组用来接收它所检测到的就绪事件。

epoll采用回调函数的方式，一旦有注册的fd就绪，触发回调函数，将就绪的fd和事件拷贝到所制定的event数组。
这样索引就绪文件的时间复杂度是O(1)，而select和poll是O(n)，因为他们需要遍历整个fds。

> 回调函数是一种软件设计上的概念 callback function
> 所谓回调函数，就是回头调用的意思 这个回调的函数会作为参数传递到某个函数内部
> 在这个函数执行中，当满足了你需要的条件，去执行这个传递进来的函数。此为callback。
> 这样就提供了一种灵活性，即只要改变参数传入的回调函数，就能改变这个函数的具体功能，而且这个函数内部无需更改。
> 这也降低了耦合性

对比select/poll
- epoll在内核里使用 **红黑树** 来跟踪所有待检测的事件，其增删改的时间复杂度是O(logn)。而select/poll需要在用户态先定义这个线性结构再传入内核，需要复制。
- epoll所返回的事件组只有已经就绪的事件，遍历他只需要O(1)。而select/poll需要传回整个fds，遍历他需要O(n)。不过他们都是基于轮询的。

### ET与LT触发模式

epoll支持两种事件触发模式：边缘触发edge-triggered,ET 水平触发level-triggered,LT

- 边缘触发：指缓冲区从边缘刚刚可读或可写时触发事件。服务端也因此只会从epoll_wait中唤醒一次。即只在刚刚可用的边缘触发。
- 水平触发：指只要缓冲区可读或可写，就触发事件。服务端也因此不断地从epoll_wait中苏醒，直到缓冲区回到空/满状态。

边缘触发下IO事件发生时只通知一次，我们要尽可能的多读写数据，防止错失读写机会。边缘触发模式一般和非阻塞I/O搭配使用。一般来说边缘触发的效率比水平触发的效率要高，因为可以减少epoll_wait的次数。而select/poll只有水平触发模式。epoll默认是水平触发的。

### ET与非阻塞模式

边缘触发只能使用非阻塞模式。

这是因为因为ET只通知一次的特性使我们必须使用while循环读完所有数据。
以读为例，read把内核缓冲区的数据挪到buffer。
- 假如内核缓冲区堆积的数据小于buffer,那么只需要一次read就可以完成读操作，并重新触发事件。
  此时ET与LT，阻塞与非阻塞都没有区别，它们都只触发一次事件。
- 而假如内核缓冲区里堆积了大于buffer的数据，
  如果使用LT模式，只需要每次事件read一次即可，因为缓冲区剩下的数据满足LT的事件触发条件。
  而如果使用ET模式，我们必须让内核缓冲区堆积的数据清空才能再次达成触发事件的条件。
  这就需要用到while循环read。而如果采用阻塞IO，那么当缓冲区被读空，read函数将导致进程阻塞。
  尽管此时也满足了事件触发的条件，但此时的阻塞由read造成，并卡死了整个进程。
- 非阻塞IO + ET模式在上述循环中，只需要判断read的返回值，查看是否已经读完数据(return -1)并在恰当的时机退出循环即可。

读取大文件几乎都采用ET非阻塞模式。

## 主流网络模型

网络模型只是一些架构上的思想和理念。就好比设计模式之于C++。

### thread-baed架构

即阻塞式IO，多线程架构，适用于并发量并不高的场景。一个连接对应一个线程。
问题如下：
1. 线程的创建、销毁开销较大
2. 线程需要占用系统资源
3. 线程切换需要开销
4. 一个进程能创建的线程有限

## reactor

- 单reactor单线程模型
  reactor直译反应堆，一个epoll对象对应一个reactor。
  此处的单线程主要针对IO操作，即accept,read,wrtie都在一个线程中完成。
  除了IO操作，业务逻辑也在这个Reactor线程上。
  所以如果业务逻辑比较耗时，就会大大降低IO请求的处理效率。

- 单reactor线程池模型
  在单reactor基础上引入线程池，用来处理逻辑操作。
虽然引入线程池后IO响应速度提升了，但在管理大数据量，高并发时单个Reactor线程效率仍然会比较低下。

- multi-reactor多线程模型
  保留了single-reactor引入的线程池外，拓展了reactor线程，引入了多个reactor线程。也称主从结构。
  mainReactor主要处理接受连接处理，subReactor主要负责客户端的处理。
  mainReactor通过一些负载均衡的策略把链接分发给其他的subReactor，后续的处理全部交给subReactor。
  同时沿用线程池，使用公共线程处理业务逻辑。